# Formation OpenStack

![](images/openstack.png)

### Concernant ces supports de cours

Supports de cours OpenStack réalisés par Osones (<https://osones.com/>)

![](images/logo-osones-new.png)

-   Copyright © 2014-2016 Osones
-   Licence : Creative Commons BY-SA 4.0
-   Sources : <https://github.com/Osones/OpenStack-Formations>

![<https://creativecommons.org/licenses/by-sa/4.0/deed.fr>](images/licence.png)

### Auteurs

-   Adrien Cunin <adrien.cunin@osones.com>
-   Pierre Freund <pierre.freund@osones.com>
-   Romain Guichard <romain.guichard@osones.com>
-   Kevin Lefevre <kevin.lefevre@osones.com>


# Introduction

### Objectifs de la formation : Cloud

-   Comprendre les principes du cloud et son intérêt
-   Connaitre le vocabulaire inhérent au cloud
-   Avoir une vue d’ensemble sur les solutions existantes en cloud public et privé
-   Posséder les clés pour tirer partie au mieux de l’IaaS
-   Pouvoir déterminer ce qui est compatible avec la philosophie cloud ou pas
-   Adapter ses méthodes d’administration système à un environnement cloud

### Objectifs de la formation : OpenStack

-   Connaitre le fonctionnement du projet OpenStack et ses possibilités
-   Comprendre le fonctionnement de chacun des composants d’OpenStack
-   Pouvoir faire les bons choix de configuration
-   Savoir déployer manuellement un cloud OpenStack pour fournir de l’IaaS
-   Connaitre les bonnes pratiques de déploiement d’OpenStack
-   Être capable de déterminer l’origine d’une erreur dans OpenStack
-   Savoir réagir face à un bug

### Pré-requis de la formation

-   Compétences d’administration système Linux tel qu’Ubuntu
    -   Gestion des paquets
    -   LVM et systèmes de fichiers
-   Notions de virtualisation, KVM et libvirt
-   Réseau : iptables, namespaces
-   Peut servir :
    -   À l’aise dans un environnement Python

# Le Cloud : vue d’ensemble

### Le Cloud : les concepts

### Le cloud, c’est large !

-   Stockage/calcul distant (on oublie, cf. externalisation)
-   Virtualisation++
-   Abstraction du matériel (voire plus)
-   Accès normalisé par des APIs
-   Service et facturation à la demande
-   Flexibilité, élasticité

### WaaS : Whatever as a Service

-   Principalement
    -   IaaS : Infrastructure as a Service
    -   PaaS : Platform as a Service
    -   SaaS : Software as a Service
-   Mais aussi :
    -   Database as a Service
    -   Network as a Service
    -   Load balancing as a Service
    -   \$APPLICATION as a Service

### Cloud public ou cloud privé ?

Public

:   fourni par un hébergeur à des clients (AWS, Rackspace Cloud, etc.)

Privé

:   plateforme et ressources internes

Hybride

:   utilisation de ressources publiques au sein d’un cloud privé

### Le cloud en un schéma

![](images/cloud.png)

### Pourquoi du cloud ? Côté business

-   Baisse des coûts par la mutualisation des ressources
-   Utilisation uniquement des ressources qui sont nécessaires
-   Va de pair avec l’automatisation du SI, l’agilité
-   Donc réduction des délais et reproductibilité
-   À grande échelle et/ou en cloud hybride, garantie de service

### Pourquoi du cloud ? Côté technique

-   Abstraction des couches plus basses
-   On peut tout programmer à son gré
-   Permet la mise en place d’architectures scalables

### PaaS : Platform as a Service

### PaaS : les principes

-   Fourniture d’une plateforme de développement
-   Fourniture d’une plateforme de déploiement
-   Pour un langage / un framework
-   Principalement utilisé par des développeurs

### Exemples de PaaS publics

-   Amazon OpsWork / Elastic Beanstalk
-   Google App Engine
-   Heroku

### Solutions de PaaS privé

-   Cloud Foundry
-   OpenShift (Red Hat)
-   Solum

### IaaS : Infrastructure as a Service

### Amazon Web Services (AWS) et les autres

-   Service (cloud public) :
    -   Pionnier du genre (dès 2006)
    -   Elastic Compute Cloud ()
    -   Elastic Block Storage ()
    -   Simple Storage Service ()

-   Logiciels libres permettant le déploiement d’un cloud privé :

    -   Eucalyptus
    -   CloudStack
    -   OpenNebula
    -   **OpenStack**

### Les clouds publics concurrents d’AWS

-   Google Compute Platform
-   Microsoft Azure
-   Rackspace
-   HP Cloud

-   En France

    -   Cloudwatt
    -   Numergy
    -   Outscale
    -   OVH
    -   Ikoula
    -   Scaleway (Iliad)

### Virtualisation dans le cloud

-   Le cloud IaaS repose souvent sur la virtualisation
-   Ressources compute $\leftarrow$ virtualisation
-   Virtualisation complète : KVM, Xen
-   Virtualisation containers : OpenVZ, LXC, Docker

### Notions et vocabulaire IaaS

-   Images
-   Instances
-   Types d’instance / gabarits (flavors)
-   Volumes
-   Stockage block
-   Stockage objet
-   IP flottantes/élastiques
-   Groupes de sécurité
-   Paires de clés
-   Templates, stacks
-   API REST
-   API de metadata et user data
-   cloud-init, cloud-config

### Notions et vocabulaire IaaS

-   L’instance est par définition éphémère
-   Elle doit être utilisée comme ressource de calcul
-   Une image se personnalise lors de son instanciation grâce à l’API de metadata
-   Séparer les données des instances
-   Choix du type de stockage : éphémère, volume, objet

### Stockage : block, objet, SDS

### Stockage block

-   Accès à des raw devices type */dev/vdb*
-   Possibilité d’utiliser n’importe quel système de fichiers
-   Compatible avec toutes les applications legacy

### Stockage objet

-   Pousser et retirer des objets dans un container/bucket
-   Pas de hiérarchie des données, pas de système de fichiers
-   Accès par les APIs
-   L’application doit être conçue pour tirer partie du stockage objet

### Stockage objet : schéma

![](images/stockage-objet.png)

### SDS : Software Defined Storage

-   Utilisation de commodity hardware
-   Pas de RAID matériel
-   Le logiciel est responsable de garantir les données
-   Les pannes matérielles sont prises en compte et gérées

### Deux solutions : OpenStack Swift et Ceph

-   Swift fait partie du projet OpenStack et fournit du stockage objet
-   Ceph fournit du stockage objet, block et FS
-   Les deux sont implémentés en SDS
-   Théorème CAP : on en choisit deux

### Théorème CAP

![](images/cap.jpg)

### Swift

-   Swift est un projet OpenStack
-   Le projet est né chez Rackspace avant la création d’OpenStack
-   Swift est en production chez Rackspace depuis lors
-   C’est le composant le plus mature d’OpenStack

### Ceph

-   Projet totalement parallèle à OpenStack
-   Supporté par une entreprise (Inktank) récemment rachetée par Red Hat
-   Fournit d’abord du stockage objet
-   L’accès aux données se fait via RADOS :
    -   Accès direct depuis une application avec librados
    -   Accès via une API REST grâce à radosgw
-   La couche RBD permet d’accéder aux données en mode block (volumes)
-   CephFS permet un accès par un système de fichiers POSIX

### Orchestrer les ressources de son IaaS

### Pourquoi orchestrer

-   Définir tout une infrastructure dans un seul fichier texte
-   Être en capacité d’instancier une infrastructure entière en un appel API
-   Autoscaling
    -   Adapter ses ressources en fonction de ses besoins en temps réel
    -   Fonctionnalité incluse dans le composant d’orchestration d’OpenStack

### APIs : quel rôle ?

### API ?

-   *Application Programming Interface*
-   Au sens logiciel : Interface permettant à un logiciel d’utiliser une bibliothèque
-   Au sens cloud : Interface permettant à un logiciel d’utiliser un service (XaaS)
-   Il s’agit le plus souvent d’API HTTP REST

### Exemple concret

    GET /v2.0/networks/<network_id>
    {
       "network":{
          "status":"ACTIVE",
          "subnets":[
             "54d6f61d-db07-451c-9ab3-b9609b6b6f0b"
          ],
          "name":"private-network",
          "provider:physical_network":null,
          "admin_state_up":true,
          "tenant_id":"4fd44f30292945e481c7b8a0c8908869",
          "provider:network_type":"local",
          "router:external":true,
          "shared":true,
          "id":"d32019d3-bc6e-4319-9c1d-6722fc136a22",
          "provider:segmentation_id":null
       }
    }

# OpenStack : projet, logiciel et utilisation

### Présentation du projet et du logiciel

### Vue haut niveau

![](images/openstack-software-diagram.pdf)

### Historique

-   Démarrage en 2010
-   Objectif : le Cloud Operating System libre
-   Fusion de deux projets de Rackspace (Storage) et de la NASA (Compute)
-   Développé en Python et distribué sous licence Apache 2.0
-   Les releases jusqu’à aujourd’hui :

    -   Austin (2010.1)
    -   Bexar (2011.1)
    -   Cactus (2011.2)
    -   Diablo (2011.3)
    -   Essex (2012.1)
    -   Folsom (2012.2)
    -   Grizzly (2013.1)
    -   Havana (2013.2)
    -   Icehouse (2014.1)
    -   Juno (2014.2)
    -   Kilo (2015.1)
    -   **Liberty (2015.2)**
    -   Avril 2016 : Mitaka

### Statistiques : Kilo

-   1492 contributeurs (Liberty : 1933)
-   169 organisations
-   394 nouvelles fonctionnalités et 7257 bugs corrigés
-   113 drivers/plugins
-   792200 chaines traduites

Source :
<http://lists.openstack.org/pipermail/foundation-board/2015-April/000050.html>

### Quelques soutiens/contributeurs ...

-  NASA
-  VMWare
-  Bull
-  Mais aussi : eNovance (racheté par Red Hat), Hastexo, StackOps
-   et **Google** ! (depuis juillet 2015)

<http://www.openstack.org/foundation/companies/>

### ... et utilisateurs

-   Tous les contributeurs précédemment cités
-   En France : **Cloudwatt** et **Numergy**
-   Wikimedia
-   CERN
-   Paypal
-   Comcast
-   BMW
-   Etc. Sans compter les implémentations confidentielles

<http://www.openstack.org/user-stories/>

### Les différents sous-projets

-   OpenStack Compute - Nova
-   OpenStack (Object) Storage - Swift
-   OpenStack Block Storage - Cinder
-   OpenStack Networking - Neutron
-   OpenStack Image Service - Glance
-   OpenStack Identity Service - Keystone
-   OpenStack Dashboard - Horizon
-   OpenStack Telemetry - Ceilometer
-   OpenStack Orchestration - Heat
-   OpenStack Database Service - Trove

### Les différents sous-projets (2)

-   Mais aussi :
    -   Bare metal (Ironic)
    -   Queue service (Zaqar)
    -   Data processing (Sahara)
    -   DNS service (Designate)
    -   Shared File Systems (Manila)
    -   Key management (Barbican)
    -   PaaS (Solum)
    -   Container (Magnum)

-   Autres

    -   Oslo
    -   rootwrap : wrapper pour les commandes root utilisé par les projets
    -   TripleO
    -   Tempest, Grenade
    -   Les clients (python-\*client)

### Cycle de vie des projets au sein d’OpenStack

![](images/innovation1.pdf)

### Les projets dans Icehouse

![](images/innovation2.pdf)

### Le nouveau modèle “big tent”

-   Évolutions presque totalement implémentées
-   Objectif : résoudre les limitations du modèle incubation/intégré
-   Inclusion a priori de l’ensemble de l’écosystème OpenStack
-   *Programs* $\rightarrow$ *Project Teams*
-   Disparation des statuts en incubation et intégré
-   Utilisation de tags factuels et objectifs

### Traduction

-   La question de la traduction est dorénavant prise en compte (officialisation de l’équipe *i18n*)
-   Seules certaines parties sont traduites, comme Horizon
-   La traduction française est aujourd’hui une des plus avancées
-   Utilisation de la plateforme Transifex (en cours de migration vers l’outil libre Zanata)

### Développement du projet : les principes

-   Open Source
-   Open Design
-   Open Development
-   Open Community

### Développement du projet : en détails

-   Ouvert à tous (individuels et entreprises)
-   Cycle de développement de 6 mois débuté par un (design) summit
-   Outils : Launchpad $\rightarrow$ Storyboard (blueprints, bugs) + Git + GitHub (code)
-   Sur chaque patch proposé :
    -   Revue de code (peer review) : Gerrit, <https://review.openstack.org>
    -   Intégration continue (continous integration) : Jenkins, Zuul, etc., <http://zuul.openstack.org/>
-   Développement hyper actif : 17000 commits dans Icehouse (+25%)
-   Fin 2012, création d’une entité indépendante de gouvernance : la fondation OpenStack

### La fondation OpenStack

-   Entité de gouvernance principale du projet
-   Représentation juridique du projet
-   Les membres du board sont issus des entreprises sponsors et élus par     les membres individuels
-   Tout le monde peut devenir membre individuel (gratuitement)
-   La fondation supporte le projet par différents moyens :
    -   Événements : organisation (Summits) ou participation (OSCON,         etc.)
    -   Infrastructure de développement (serveurs)
    -   Ressources humaines : marketing, release manager, quelques         développeurs (principalement sur l’infrastructure)
-   500 organisations à travers le monde
-   23000 membres individuels dans 160 pays

### La fondation OpenStack

![](images/foundation.png)

### OpenStack Infra

-   Équipe projet en charge de l’infrastructure de développement d’OpenStack
-   Travaille comme les équipes de dev d’OpenStack et utilise les mêmes outils
-   Résultat : une infrastructure entièrement open source et développée comme tel

### Cycle de développement : 6 mois

-   Le planning est publié, exemple : <https://wiki.openstack.org/wiki/Mitaka_Release_Schedule>
-   Milestone releases
-   Freezes : FeatureProposal, Feature, String
-   RC releases
-   Stable releases
-   Ce modèle de cycle de développement a évolué depuis le début du projet
-   Cas particulier de Swift et de plus en plus de composants
-   Depuis Liberty, chaque composant gère son propre versionnement

### Versionnement depuis Liberty

-   *Semantic versioning*
-   Chaque projet est indépendant
-   Dans le cadre du cycle de release néanmoins
-   <http://docs.openstack.org/releases/>

### Où trouver des informations sur le développement d’OpenStack

-   Principalement sur le wiki : <https://wiki.openstack.org>
-   Les blueprints et bugs sur Launchpad/StoryBoard : <https://launchpad.net/openstack>, 
<https://storyboard.openstack.org>, <http://specs.openstack.org/>
-   Les patchs proposés et leurs reviews sont sur Gerrit <https://review.openstack.org>
-   L’état de la CI (entre autres) : <http://status.openstack.org>
-   Le code est disponible dans Git : <https://git.openstack.org>
-   Les sources (tarballs) sont disponibles : <http://tarballs.openstack.org/>

### Qui contribue ?

-   *Active Technical Contributor*
-   ATC : personne ayant au moins une contribution récente dans un projet OpenStack reconnu
-   Les ATC sont invités aux summits et ont le droit de vote
-   *Core reviewer* : ATC ayant les droits pour valider les patchs dans un projet
-   *Project Team Lead* (PTL) : élu par les ATCs de chaque projet
-   Stackalytics fournit des statistiques sur les contributions

<http://stackalytics.com/>

### Stackforge

-   Forge pour les nouveaux projets en lien avec OpenStack
-   Ils bénéficient de l’infrastructure du projet OpenStack, mais la séparation reste claire
-   Les projets démarrent dans Stackforge et peuvent ensuite rejoindre le projet OpenStack
-   En train de disparaitre au profit du modèle “big tent”

### OpenStack Summit

-   Aux USA jusqu’en 2013
-   Aujourd’hui : alternance Amérique de Nord et Asie/Europe
-   Quelques centaines au début à 4500 participants aujourd’hui
-   En parallèle : conférence (utilisateurs, décideurs) et Design Summit (développeurs)
-   Détermine le nom de la release : lieu/ville à proximité du Summit
-   *Upstream Training*

### Exemple du Summit d’avril 2013 à Portland

![](images/photo-summit.jpg)

### Exemple du Summit d’octobre 2015 à Tokyo

![](images/photo-summit1.jpg) Photo : Elizabeth K. Joseph, CC BY
2.0, Flickr/pleia2

### Exemple du Summit d’octobre 2015 à Tokyo

![](images/photo-summit2.jpg) Photo : Elizabeth K. Joseph, CC BY
2.0, Flickr/pleia2

### Exemple du Summit d’octobre 2015 à Tokyo

![](images/photo-summit3.jpg) Photo : Elizabeth K. Joseph, CC BY
2.0, Flickr/pleia2

### Exemple du Summit d’octobre 2015 à Tokyo

![](images/photo-summit4.jpg) Photo : Elizabeth K. Joseph, CC BY
2.0, Flickr/pleia2

### Design Tenets

1.  Scalability and elasticity are our main goals
2.  Any feature that limits our main goals must be optional
3.  Everything should be asynchronous. If you can’t do something asynchronously, see \#2
4.  All required components must be horizontally scalable
5.  Always use shared nothing architecture (SN) or sharding. If you can’t Share nothing/shard, see \#2
6.  Distribute everything. Especially logic. Move logic to where state naturally exists.
7.  Accept eventual consistency and use it where it is appropriate.
8.  Test everything. We require tests with submitted code. (We will help you if you need it)

### Implémentation

-   Chaque sous-projet est découpé en plusieurs services
-   Communication entre les services : AMQP (RabbitMQ)
-   Base de données : relationnelle SQL (MySQL/MariaDB)
-   Réseau : OpenVSwitch
-   En général : réutilisation de composants existants
-   Tout est développé en Python (Django pour la partie web)
-   APIs supportées : OpenStack et équivalent Amazon
-   Multi tenancy

### Multi-tenant

-   Notion générale : un déploiement du logiciel permet de multiples utilisations
-   Un cloud OpenStack permet aux utilisateurs de travailler dans des environnements isolés
-   Les instances, réseaux, images, etc. sont associés à un tenant
-   Certaines ressources peuvent être partagées entre tenants (exemple : image publique)
-   On peut aussi parler de “projet”

### Architecture

![](images/architecture-simple.jpg)

### Interface web / Dashboard : Horizon

![](images/horizon.png)

### Ressources

-   Annonces/sécurité : openstack-announce@lists.openstack.org
-   Documentation : <http://docs.openstack.org/>
-   Gouvernance du projet : <http://governance.openstack.org/>
-   Support :
    -   <https://ask.openstack.org>
    -   openstack@lists.openstack.org
    -   \#openstack@Freenode
-   SDK/APIs : <http://developer.openstack.org/>
-   Applications : <http://apps.openstack.org/>
-   Actualités :
    -   Blog officiel : <http://www.openstack.org/blog/>
    -   Planet : <http://planet.openstack.org>
    -   Superuser : <http://superuser.openstack.org/>
    -   OpenStack Community Weekly Newsletter
-   Ressources commerciales : <http://www.openstack.org/marketplace/> entre autres

### Ressources - Communauté francophone

![](images/openstackfr.png)

-   Site web : <http://openstack.fr/>
-   Association des utilisateurs francophones d’OpenStack :     <https://asso.openstack.fr/>
-   Meetups : Paris, Rhônes-Alpes, Toulouse, Montréal, ...
-   Présence à des événements tels que Solutions Linux
-   Canaux de communication :
    -   openstack-fr@lists.openstack.org
    -   \#openstack-fr@Freenode

### DevStack : faire tourner rapidement un OpenStack

### Utilité de DevStack

-   Déployer rapidement un OpenStack
-   Utilisé par les développeurs pour tester leurs changements
-   Utilisé pour faire des démonstrations
-   Utilisé pour tester les APIs sans se préoccuper du déploiement
-   Ne doit PAS être utilisé pour de la production

### Fonctionnement de DevStack

-   Un script shell qui fait tout le travail : stack.sh
-   Un fichier de configuration : local.conf
-   Installe toutes les dépendances nécessaires (paquets)
-   Clone les dépôts git (branche master par défaut)
-   Lance tous les composants dans un screen

### Configuration : local.conf

Exemple

    [[local|localrc]]
    ADMIN_PASSWORD=secrete
    DATABASE_PASSWORD=$ADMIN_PASSWORD
    RABBIT_PASSWORD=$ADMIN_PASSWORD
    SERVICE_PASSWORD=$ADMIN_PASSWORD
    SERVICE_TOKEN=a682f596-76f3-11e3-b3b2-e716f9080d50
    #FIXED_RANGE=172.31.1.0/24
    #FLOATING_RANGE=192.168.20.0/25
    #HOST_IP=10.3.4.5

### Conseils d’utilisation

-   DevStack installe beaucoup de choses sur la machine
-   Il est recommandé de travailler dans une VM
-   Pour tester tous les composants OpenStack dans de bonnes conditions, plusieurs Go de RAM sont nécessaires
-   L’utilisation de Vagrant est conseillée

### Utiliser OpenStack

### Le principe

-   Toutes les fonctionnalités sont accessibles par l’API
-   Les clients (y compris Horizon) utilisent l’API
-   Des crédentials sont nécessaires
    -   API OpenStack : utilisateur + mot de passe + tenant
    -   API AWS : access key ID + secret access key

### Accès aux APIs

-   Direct, en HTTP, via des outils comme curl
-   Avec une bibliothèque
    -   Les implémentations officielles en Python
    -   D’autres implémentations pour d’autres langages (exemple : jclouds)
-   Avec les outils officiels en ligne de commande
-   Avec Horizon

### Clients officiels

-   Le projet fournit des clients officiels : python-PROJETclient
-   Bibliothèques Python
-   Outils CLI
    -   L’authentification se fait en passant les credentials par
        paramètres ou variables d’environnement
    -   L’option –debug affiche la communication HTTP

### OpenStack Client

-   Client CLI unifié
-   Commandes du type *openstack \<service\> \<action\>*
-   Vise à remplacer à terme les clients spécifiques
-   Permet une expérience utilisateur plus homogène

# Déployer OpenStack

### Ce qu’on va voir

-   Installer OpenStack à la main <http://docs.openstack.org/juno/install-guide/install/apt/content/>
-   Donc comprendre son fonctionnement
-   Passer en revue chaque composant plus en détails
-   Tour d’horizon des solutions de déploiement

### Architecture détaillée

![](images/architecture.jpg)

### Architecture réseau

![](images/archi-network.png)

### Architecture services

![](images/archi-service.png)

### Quelques éléments de configuration généraux

-   Tous les composants doivent être configurés pour communiquer avec Keystone
-   La plupart doivent être configurés pour communiquer avec MySQL/MariaDB et RabbitMQ
-   Les composants découpés en plusieurs services ont souvent un fichier de configuration par service
-   api-paste.ini contient des paramètres concernant le service API

### Les briques nécessaires

### Système d’exploitation

-   OS Linux avec Python
-   Historiquement : Ubuntu
-   Red Hat s’est largement rattrapé
-   SUSE, etc.

### Python

![](images/python-powered.png)

-   OpenStack est aujourd’hui compatible Python 2.7
-   Afin de ne pas réinventer la roue, beaucoup de dépendances sont nécessaires
-   Un travail de portage vers Python 3 est en cours

### Base de données

-   Permet de stocker la majorité des données gérées par OpenStack
-   Chaque composant a sa propre base
-   OpenStack utilise l’ORM Python SQLAlchemy
-   Support théorique équivalent à celui de SQLAlchemy
-   MySQL/MariaDB est l’implémentation la plus largement testée et utilisée
-   SQLite est principalement utilisé dans le cadre de tests et démo
-   Certains déploiements fonctionnent avec PostgreSQL

### Pourquoi l’utilisation de SQLAlchemy

![](images/sqlalchemy-logo.png)

-   Support de multiples BDD
-   Gestion des migrations

![](images/mysql-logo.png)

### Passage de messages

-   AMQP : Advanced Message Queuing Protocol
-   Messages, file d’attente, routage
-   Les processus OpenStack communiquent via AMQP
-   Plusieurs implémentations possibles : Qpid, 0MQ, etc.
-   RabbitMQ par défaut

### RabbitMQ

![](images/rabbitmq-logo.png)

-   RabbitMQ est implémenté en Erlang
-   Une machine virtuelle Erlang est donc nécessaire

### “Hello world” RabbitMQ

![](images/rabbitmq-schema.png)

### Keystone : Authentification, autorisation et catalogue de services

### Principes

-   Annuaire des utilisateurs et des groupes
-   Catalogue de services
-   Gère l’authentification et l’autorisation
-   Support des domaines dans l’API v3
-   Fournit un token à l’utilisateur

### API

-   API admin : port 35357
-   API utilisateur : port 5000
-   Deux versions : v2 (actuelle) et v3 (future; APIs admin et utilisateur fusionnées)
-   Gère *utilisateurs*, *groupes*, *domaines* (APIv3)
-   Les utilisateurs ont des *rôles* sur des *tenants* (projets)

### Scénario d’utilisation typique

![](images/keystone-scenario.png)

### Installation et configuration

-   Paquet : keystone
-   Configuration d’un token ADMIN pour la configuration initiale
-   Backends : choix de SQL ou LDAP (ou AD)
-   Backends tokens : SQL, Memcache
-   Configurer les différents services
-   Policy.json
-   Services et endpoints
-   Utilisateurs, groupes, domaines

### Enregistrer un service et son endpoint

Il faut renseigner l’existence des différents services (catalogue) dans
Keystone :

    $ keystone service-create --name=cinderv2 --type=volumev2 \
      --description="Cinder Volume Service V2"
    $ keystone endpoint-create \
      --region=myRegion
      --service-id=...
      --publicurl=http://controller:8776/v2/%\(tenant_id\)s \
      --internalurl=http://controller:8776/v2/%\(tenant_id\)s \
      --adminurl=http://controller:8776/v2/%\(tenant_id\)s

### Tester

    $ keystone service-list
    ...
    $ keystone user-list
    ...
    $ keystone token-get
    ...

###Nova : Compute

### Principes

-   Gère les instances
-   Les instances sont créées à partir des images fournies par Glance
-   Les interfaces réseaux des instances sont associées à des ports Neutron
-   Du stockage block peut être fourni aux instances par Cinder

### Interactions avec les autres composants

![](images/compute-node.png)

### Propriétés d’une instance

-   Éphémère, a priori non hautement disponible
-   Définie par une flavor
-   Construite à partir d’une image
-   Optionnel : attachement de volumes
-   Optionnel : boot depuis un volume
-   Optionnel : une clé SSH publique
-   Optionnel : des ports réseaux

### API

Gère :

-   Instances
-   Flavors (types d’instance)
-   Indirectement : images, security groups (groupes de sécurité), floating IPs (IPs flottantes)

Les instances sont redimensionnables et migrables d’un hôte physique à un autre.

### Les groupes de sécurité

-   Équivalent à un firewall devant chaque instance
-   Une instance peut être associée à un ou plusieurs groupes de sécurité
-   Gestion des accès en entrée et sortie
-   Règles par protocole (TCP/UDP/ICMP) et par port
-   Cible une adresse IP, un réseau ou un autre groupe de sécurité

### Flavors

-   *Gabarit*
-   Équivalent des “instance types” d’AWS
-   Définit un modèle d’instance en termes de CPU, RAM, disque (racine), disque éphémère
-   Un disque de taille nul équivaut à prendre la taille de l’image de base
-   Le disque éphémère a, comme le disque racine, l’avantage d’être souvent local donc rapide

### Nova api

-   Double rôle
-   API de manipulation des instances par l’utilisateur
-   API à destination des instances : API de metadata
-   L’API de metadata doit être accessible à l’adresse http://169.254.169.254/
-   L’API de metadata fournit des informations de configuration personnalisées à chacune des instances

### Nova compute

-   Pilote les instances (machines virtuelles ou physiques)
-   Tire partie de libvirt ou d’autres APIs comme XenAPI
-   Drivers : libvirt (KVM, LXC, etc.), XenAPI, VMWare vSphere, Docker, Ironic
-   Permet de récupérer les logs de la console et une console VNC

### Nova scheduler

-   Service qui distribue les demandes d’instances sur les nœuds compute
-   Filter, Chance, Multi Scheduler
-   Filtres, par défaut : AvailabilityZoneFilter,RamFilter,ComputeFilter
-   Tri par poids, par défaut : RamWeigher

### Le scheduler Nova en action

![](images/scheduling-schema.png)

### Nova conductor

-   Service facultatif qui améliore la sécurité
-   Fait office de proxy entre les nœuds compute et la BDD
-   Les nœuds compute, vulnérables, n’ont donc plus d’accès à la BDD

### Tester

    $ nova list
    ...
    $ nova create
    ...

###Glance : Registre d’images

### Principes

-   Registre d’images (et des snapshots)
-   Propriétés sur les images
-   Est utilisé par Nova pour démarrer des instances
-   Peut utiliser Swift comme back-end de stockage

### Propriétés des images dans Glance

L’utilisateur peut définir un certain nombre de propriétés dont
certaines seront utilisées lors de l’instanciation

-   Type d’image
-   Architecture
-   Distribution
-   Version de la distribution
-   Espace disque minimum
-   RAM minimum
-   Publique ou non

### Types d’images

Glance supporte un large éventail de types d’images, limité par le
support de l’hyperviseur sous-jacent à Nova

-   raw
-   qcow2
-   ami
-   vmdk
-   iso

### Backends

-   Swift ou S3
-   Ceph
-   HTTP
-   Répertoire local

### Installation

-   Paquet glance-api : fournit l’API
-   Paquet glance-registry : démon du registre d’images en tant que tel

### Tester

    $ glance image-list
    ...
    $ glance image-create
    ...

###Neutron : Réseau en tant que service

### Principes

-   *Software Defined Networking* (SDN)
-   Auparavant Quantum et nova-network
-   IP flottantes, groupes de sécurité
-   neutron-server : fournit l’API
-   Agent DHCP : fournit le service de DHCP pour les instances
-   Agent L3 : gère la couche 3 du réseau, le routage
-   Plugin : OpenVSwitch par défaut, d’autres implémentations
    libres/propriétaires, logicielles/matérielles existent

### Fonctionnalités supplémentaires

Outre les fonctions réseau de base niveaux 2 et 3, Neutron peut fournir
d’autres services :

-   Load Balancing (HAProxy, ...)
-   Firewall (vArmour, ...) : diffère des groupes de sécurité
-   VPN (Openswan, ...) : permet d’accéder à un réseau privé sans IP flottantes

Ces fonctionnalités se basent également sur des plugins

### API

L’API permet notamment de manipuler ces ressources

-   Réseau (*network*) : niveau 2
-   Sous-réseau (*subnet*) : niveau 3
-   Port : attachable à une interface sur une instance, un load-balancer, etc.
-   Routeur

### Plugins ML2

-   Modular Layer 2
-   OpenVSwitch
-   OpenDaylight
-   Contrail, OpenContrail
-   Nuage Networks
-   VMWare NSX
-   cf. OpenFlow

### Implémentation

-   Neutron tire partie des namespaces réseaux du noyau Linux pour    permettre l’IP overlapping
-   Le proxy de metadata est un composant qui permet aux instances isolées dans leur réseau de joindre l’API de metadata fournie par Nova

### Schéma

![](images/neutron-schema.png)

### Schéma

![](images/neutron-schema2.png)

###Cinder : Stockage block

### Principes

-   Auparavant nova-volume
-   Fournit des volumes (stockage block) attachables aux instances
-   Gère différents types de volume
-   Gère snapshots et backups de volumes
-   Attachement via iSCSI par défaut

### Du stockage partagé ?

-   Cinder n’est **pas** une solution de stockage partagé comme NFS
-   Le projet OpenStack Manila a pour objectif d’être un *NFS as a Service*
-   AWS n’a introduit une telle fonctionnalité que récemment

### Utilisation

-   Volume supplémentaire (et stockage persistant) sur une instance
-   Boot from volume : l’OS est sur le volume
-   Fonctionnalité de backup vers un object store (Swift ou Ceph)

### Installation

-   Paquet cinder-api : fournit l’API
-   Paquet cinder-volume : création et gestion des volumes
-   Paquet cinder-scheduler : distribue les demandes de création de volume
-   Paquet cinder-backup : backup vers un object store

### Backends

-   Utilisation de plusieurs backends en parallèle possible
-   LVM (par défaut)
-   GlusterFS
-   Ceph
-   Systèmes de stockage propriétaires type NetApp
-   DRBD

###Horizon : Dashboard web

### Principes

-   Utilise les APIs existantes pour fournir une interface utilisateur
-   Horizon est un module Django
-   OpenStack Dashboard est l’implémentation officielle de ce module

![](images/django-logo.png)

### Configuration

-   local\_settings.py
-   Les services apparaissent dans Horizon s’ils sont répertoriés dans
    le catalogue de services de Keystone

### Utilisation

-   Une zone “admin” restreinte
-   Une interface par tenant

Swift : Stockage objet

### Principes

-   SDS : Software Defined Storage
-   Utilisation de commodity hardware
-   Théorème CAP : on en choisit deux
-   Accès par les APIs
-   Architecture totalement acentrée
-   Pas de base de données centrale

### Implémentation

-   Proxy : serveur API par lequel passent toutes les requêtes
-   Object server : serveur de stockage
-   Container server : maintient des listes d’objects dans des containers
-   Account server : maintient des listes de containers dans des accounts
-   Chaque objet est répliqué n fois (3 par défaut)

### Le ring

-   Problème : comment décider quelle donnée va sur quel object server
-   Le ring est découpé en partitions
-   On situe chaque donnée dans le ring afin de déterminer sa partition
-   Une partition est associée à plusieurs serveurs

### Schéma

![](images/swift-schema.png)

###Ceilometer : Collecte de métriques

### Surveiller l’utilisation de son infrastructure avec Ceilometer

-   Indexe différentes métriques concernant l’utilisation des différents services du cloud
-   Fournit des APIs permettant de récupérer ces données
-   Base pour construire des outils de facturation (exemple : CloudKitty)
-   Utilise MongoDB (par défaut) pour le stockage

### Gnocchi : time-series database

-   Pourquoi Gnocchi ? Palier aux problème de scalabilité de Ceilometer
-   Initié par des développeurs de Ceilometer et intégré à l’équipe projet Ceilometer
-   Back-end remplaçant MongoDB pour Ceilometer

###Heat : Orchestration des ressources

### Orchestrer son infrastructure avec Heat

-   Équivalent d’Amazon CloudFormation
-   Orchestrer les ressources compute, storage, network, etc.
-   Doit se coupler avec cloud-init
-   Description de son infrastructure dans un fichier template, format JSON (CFN) ou YAML (HOT)

### Autoscaling avec Heat

Heat implémente la fonctionnalité d’autoscaling

-   Se déclenche en fonction des alarmes produites par Ceilometer
-   Entraine la création de nouvelles instances

### Un template HOT

*parameters* - *resources* - *outputs*

    heat_template_version: 2013-05-23

    description: Simple template to deploy a single compute instance

    resources:
      my_instance:
        type: OS::Nova::Server
        properties:
          key_name: my_key
          image: F18-x86_64-cfntools
          flavor: m1.small

### Fonctionnalités avancées de Heat

-   Nested stacks
-   Environments
-   Providers

### Construire un template à partir d’existant

Multiples projets en cours de développement :

-   Flame (Cloudwatt)
-   HOT builder
-   Merlin

###Trove : Database as a Service

### Principe

-   Fournit des bases de données relationnelles, à la AWS RDS
-   A vocation à supporter des bases NoSQL aussi
-   Gère notamment MySQL/MariaDB comme back-end
-   Se repose sur Nova pour les instances dans lesquelles se fait l’installation d’une BDD

### Services

-   trove-api : API
-   trove-taskmanager : gère les instances BDD
-   trove-guestagent : agent interne à l’instance

###Designate : DNS as a Service

### Principe

-   Équivalent d’AWS Route 53
-   Gère différents backends : BIND, etc.

###Quelques autres composants intéressants

### Ironic

-   Anciennement Nova bare-metal
-   Permet le déploiement d’instance sur des machines physiques (plutôt que VMs)
-   Repose sur des technologies telles que PXE, TFTP

### Oslo, ou OpenStack common

-   Oslo contient le code commun à plusieurs composants d’OpenStack
-   Son utilisation est transparente pour le déployeur

### rootwrap

-   Wrapper pour l’utilisation de commandes en root
-   Configuration au niveau de chaque composant qui l’utilise
-   Permet d’écrire des filtres sur les commandes

### TripleO

-   OpenStack On OpenStack
-   Objectif : pouvoir déployer un cloud OpenStack (*overcloud*) à partir d’un cloud OpenStack (*undercloud*)
-   Autoscaling du cloud lui-même : déploiement de nouveaux nœuds compute lorsque cela est nécessaire
-   Fonctionne conjointement avec Ironic pour le déploiement bare-metal

### Tempest

-   Suite de tests d’un cloud OpenStack
-   Effectue des appels à l’API et vérifie le résultat
-   Est très utilisé par les développeurs via l’intégration continue
-   Le déployeur peut utiliser Tempest pour vérifier la bonne conformité
    de son cloud

###Bonnes pratiques pour un déploiement en production

### Quels composants dois-je installer ?

-   Keystone est indispensable
-   L’utilisation de Nova va de paire avec Glance et Neutron
-   Cinder s’avérera souvent utile
-   Ceilometer et Heat vont souvent ensemble
-   Swift est indépendant des autres composants
-   Neutron peut parfois être utilisé indépendamment (ex : avec oVirt)

<http://docs.openstack.org/arch-design/content/>

### Penser dès le début aux choix structurants

-   Distribution et méthode de déploiement
-   Hyperviseur
-   Réseau : quelle architecture et quels drivers
-   Politique de mise à jour

### Les différentes méthodes d’installation

-   DevStack est à oublier pour la production
-   TripleO est encore jeune
-   Le déploiement à la main comme vu précédemment n’est pas recommandé     car peu maintenable
-   Distributions OpenStack packagées et prêtes à l’emploi
-   Distributions classiques et gestion de configuration
-   Déploiement continu

### Mises à jour entre versions majeures

-   OpenStack supporte les mises à jour N $\rightarrow$ N+1
-   Swift : très bonne gestion en mode *rolling upgrade*
-   Autres composants : tester préalablement avec vos données
-   Lire les release notes
-   Cf. articles de blog du CERN

### Mises à jour dans une version stable

-   Fourniture de correctifs de bugs majeurs et de sécurité
-   OpenStack intègre ces correctifs sous forme de patchs dans la branche stable
-   Publication de *point releases* intégrant ces correctifs issus de la branche stable
-   Durée variable du support des versions stables, dépendant de l’intérêt des intégrateurs

### Assigner des rôles aux machines

Beaucoup de documentations font référence à ces rôles :

-   Controller node : APIs, BDD, AMQP
-   Network node : DHCP, routeur, IPs flottantes
-   Compute node : Hyperviseur/pilotage des instances

Ce modèle simplifié n’est pas HA.

### Haute disponibilité

Haut disponibilité de l’IaaS

-   MySQL/MariaDB, RabbitMQ : HA classique (Galera, Clustering)
-   Les services APIs sont stateless et HTTP : scale out et load balancers
-   La plupart des autres services OpenStack sont capables de scale out également

Guide HA : <http://docs.openstack.org/high-availability-guide/content/>

### Haute disponibilité de l’agent L3 de Neutron

-   Plusieurs solutions et contournements possibles
-   Depuis Juno : *Distributed Virtual Router* (DVR)

### Considérations pour une environnement de production

-   Des URLs uniformes pour toutes les APIs : utiliser un reverse proxy
-   Apache/mod\_wsgi pour servir les APIs lorsque cela est possible (Keystone)
-   Utilisation des quotas
-   Prévoir les bonnes volumétries, notamment pour les données Ceilometer
-   Monitoring
-   Backup
-   QoS : en cours d’implémentation dans Neutron

Guide Operations : <http://docs.openstack.org/trunk/openstack-ops/content/>

### Utilisation des quotas

-   Limiter le nombre de ressources allouables
-   Par utilisateur ou par tenant
-   Support dans Nova
-   Support dans Cinder
-   Support dans Neutron

<http://docs.openstack.org/user-guide-admin/content/cli_set_quotas.html>

### Découpage réseau

-   Management network : réseau d’administration
-   Data network : réseau pour la communication inter instances
-   External network : réseau externe, dans l’infrastructure réseau existante
-   API network : réseau contenant les endpoints API

### Considérations liées à la sécurité

-   Indispensable : HTTPS sur l’accès des APIs à l’extérieur
-   Sécurisation des communications MySQL/MariaDB et RabbitMQ
-   Un accès MySQL/MariaDB par base et par service
-   Un utilisateur Keystone par service
-   Limiter l’accès en lecture des fichiers de configuration (mots de passe, token)
-   Veille sur les failles de sécurité : OSSA (*OpenStack Security Advisory*), OSSN (*... Notes*)

Guide sécurité : <http://docs.openstack.org/security-guide/content/>

### Segmenter son cloud

-   Host aggregates : machines physiques avec des caractéristiques similaires
-   Availability zones : machines dépendantes d’une même source électrique, d’un même switch, d’un même DC, etc.
-   Regions : chaque région a son API
-   Cells : permet de regrouper plusieurs cloud différents sous une même API

<http://docs.openstack.org/openstack-ops/content/scaling.html#segregate_cloud>

### Host aggregates / agrégats d’hôtes

-   Spécifique Nova
-   L’administrateur définit des agrégats d’hôtes via l’API
-   L’administrateur associe flavors et agrégats via des couples clé/valeur communs
-   1 agrégat $\equiv$ 1 point commun, ex : GPU
-   L’utilisateur choisit un agrégat à travers son choix de flavor à la création d’instance

### Availability zones / zones de disponibilité

-   Spécifique Nova et Cinder
-   Groupes d’hôtes
-   Découpage en termes de disponibilité : Rack, Datacenter, etc.
-   L’utilisateur choisit une zone de disponibilité à la création d’instance
-   L’utilisateur peut demander à ce que des instances soient démarrées dans une même zone, ou au contraire dans des zones différentes

### Régions

-   Générique OpenStack
-   Équivalent des régions d’AWS
-   Un service peut avoir différents endpoints dans différentes régions
-   Chaque région est autonome
-   Cas d’usage : cloud de grande ampleur (comme certains clouds
    publics)

### Cells / Cellules

-   Spécifique Nova
-   Un seul nova-api devant plusieurs cellules
-   Chaque cellule a sa propre BDD et file de messages
-   Ajoute un niveau de scheduling (choix de la cellule)

### Packaging d’OpenStack - Ubuntu

-   Le packaging est fait dans de multiples distributions, RPM, DEB et
    autres
-   Ubuntu est historiquement la plateforme de référence pour le
    développement d’OpenStack
-   Le packaging dans Ubuntu suit de près le développement d’OpenStack,
    et des tests automatisés sont réalisés
-   Canonical fournit la Ubuntu Cloud Archive, qui met à disposition la
    dernière version d’OpenStack pour la dernière Ubuntu LTS

### Ubuntu Cloud Archive

![](images/ubuntu-cloud-archive.png)

### Packaging d’OpenStack dans les autres distributions

-   OpenStack est intégré dans les dépôts officiels de Debian
-   Red Hat propose RHOS/RDO
-   Comme Ubuntu, le cycle de release de Fedora est synchronisé avec
    celui d’OpenStack

### Les distributions OpenStack

-   RedHat OpenStack Plateform
-   Mirantis
-   HP Helion
-   etc.

### Déploiement bare metal

-   Le déploiement des hôtes physiques OpenStack peut se faire à l’aide d’outils dédiés
-   MaaS (Metal as a Service), par Ubuntu/Canonical : se couple avec Juju
-   Crowbar / OpenCrowbar (initialement Dell) : utilise Chef
-   eDeploy (eNovance) : déploiement par des images
-   Ironic via TripleO

### Gestion de configuration

-   Puppet, Chef, CFEngine, Saltstack, Ansible, etc.
-   Ces outils peuvent aider à déployer le cloud OpenStack
-   ... mais aussi à gérer les instances (section suivante)

### Modules Puppet

-   Puppet Labs maintient (avec d’autres) des modules pour déployer    OpenStack
-   <https://forge.puppetlabs.com/puppetlabs/openstack>

### Déploiement continu

-   OpenStack maintient un master (trunk) toujours stable
-   Possibilité de déployer au jour le jour le master (CD: *Continous Delivery*)
-   Nécessite la mise en place d’une infrastructure importante
-   Facilite les mises à jour entre versions majeures

###Faire face aux problèmes

### Ressources FAILED/ERROR

-   Plusieurs causes possibles
-   Possibilité de supprimer la ressource ?
-   L’appel API *reset-state* peut servir

### Les réflexes en cas d’erreur ou de comportement erroné

-   Travaille-t-on sur le bon tenant ?
-   Est-ce que l’API renvoie une erreur ? (le dashboard peut cacher    certaines informations)
-   Si nécessaire d’aller plus loin :
    -   Regarder les logs sur le cloud controller (/var/log/\<composant\>/\*.log)
    -   Regarder les logs sur le compute node et le network node si le problème est spécifique réseau/instance
    -   Éventuellement modifier la verbosité des logs dans la configuration

### Est-ce un bug ?

-   Si le client CLI crash, c’est un bug
-   Si le dashboard web affiche une erreur 500, c’est peut-être un bug
-   Si les logs montrent une stacktrace Python, c’est un bug
-   Sinon, à vous d’en juger

#Tirer partie de l’IaaS

### Deux visions

Une fois un cloud IaaS en place, deux optiques possibles :

-   Garder les mêmes pratiques tout en profitant du self service et de l’agilité de la solution pour des besoins test/dev
-   Faire évoluer ses pratiques, tant côté applicatif que système

“Pets vs Cattle”

### Sinon ?

Faire tourner des applications *legacy* dans le cloud est une mauvaise
idée :

-   Interruptions de service
-   Pertes de données
-   Incompréhensions “le cloud ça marche pas”

###Côté applications

### Adapter ou penser ses applications “cloud ready”

Cf. les design tenets du projet OpenStack et Twelve-Factor
<http://12factor.net/>

-   Architecture distribuée plutôt que monolithique
    -   Facilite le passage à l’échelle
    -   Limite les domaines de *failure*
-   Couplage faible entre les composants
-   Bus de messages pour les communications inter-composants
-   Stateless : permet de multiplier les routes d’accès à l’application
-   Dynamicité : l’application doit s’adapter à son environnement et se reconfigurer lorsque nécessaire
-   Permettre le déploiement et l’exploitation par des outils d’automatisation
-   Limiter autant que possible les dépendances à du matériel ou du logiciel spécifique qui pourrait ne pas fonctionner dans un cloud
-   Tolérance aux pannes (*fault tolerance*) intégrée
-   Ne pas stocker les données en local, mais plutôt :
    -   Base de données
    -   Stockage objet
-   Utiliser des outils standards de journalisation

###Côté système

### Adopter une philosophie DevOps

-   Infrastructure as Code
-   Scale out plutôt que scale up (horizontalement plutôt que verticalement)
-   HA niveau application plutôt qu’infrastructure
-   Automatisation, automatisation, automatisation
-   Tests

### Monitoring et backup

Monitoring

-   Prendre en compte le cycle de vie des instances
-   Monitorer le service plus que le serveur

Backuper, quoi ?

-   Être capable de recréer ses instances (et le reste de son infrastructure)
-   Données (applicatives, logs) : block, objet

### Utiliser des images cloud

Une image cloud c’est :

-   Une image disque contenant un OS déjà installé
-   Une image qui peut être instanciée en n machines sans erreur
-   Un OS sachant parler à l’API de metadata du cloud (cloud-init)

Détails :
<http://docs.openstack.org/image-guide/content/ch_openstack_images.html>\
La plupart des distributions fournissent aujourd’hui des images cloud.

### Cirros

-   Cirros est l’image cloud par excellence
-   OS minimaliste
-   Contient cloud-init

<https://launchpad.net/cirros>

### Cloud-init

-   Cloud-init est un moyen de tirer partie de l’API de metadata, et notamment des user data
-   L’outil est intégré par défaut dans la plupart des images cloud
-   À partir des user data, cloud-init effectue les opérations de personnalisation de l’instance
-   cloud-config est un format possible de user data

### Exemple cloud-config

    #cloud-config
    mounts:
     - [ xvdc, /var/www ]
    packages:
     - apache2
     - htop

### Comment gérer ses images ?

-   Utilisation d’images génériques et personnalisation à l’instanciation
-   Création d’images intermédiaires et/ou totalement personnalisées :
    *Golden images*
    -   libguestfs, virt-builder, virt-sysprep
    -   diskimage-builder (TripleO)
    -   Packer
    -   solution “maison”

### Configurer et orchestrer ses instances

-   Outils de gestion de configuration (les mêmes qui permettent de déployer OpenStack)
-   Juju

# Conclusion

-   Le cloud révolutionne l’IT
-   OpenStack est le projet libre phare sur la partie IaaS
-   Déployer OpenStack n’est pas une mince affaire
-   L’utilisation d’un cloud IaaS implique des changements de pratique

